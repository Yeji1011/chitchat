from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

import torch

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
print('Available device : ', torch.cuda.device_count())
print('Current cuda device :', torch.cuda.current_device())
print(torch.cuda.get_device_name(device))
#cuda
#Available device :  1
#Current cuda device : 0
#Quadro RTX 8000




# Setting parameters
batch_size = 100
num_label = 2
learning_rate = 1e-5
max_grad_norm = 1
# epochs = 1
num_workers = 8
# max_len = 512



import time
start_time = time.time()

from transformers import BertTokenizer, RobertaTokenizer, RobertaForSequenceClassification
import torch

config = 'klue/roberta-large'
tokenizer = BertTokenizer.from_pretrained(config)
model = RobertaForSequenceClassification.from_pretrained(config, num_labels=num_label)

# model.to(device)

model_save_dir='/home/yeji/standard/model/chitchat_classification/chitchat_classification/Mix_5sentences3/experiment_dataset2/saved_model/epoch_1.pt'

learning_rate = 1e-5
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
checkpoint = torch.load(model_save_dir)

# 원래 module. 없어야됨
# 근데 생겨서 지움
new_checkpoint = { k.replace('module.','') if 'module.' in k else k:v for k,v in checkpoint['model_state_dict'].items()}

model.load_state_dict(new_checkpoint)#checkpoint['model_state_dict'])
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
epoch = checkpoint['epoch']
model.to(device)

model.eval()
print('Model loaded!')



import pandas as pd

test_path ='/home/yeji/standard/model/chitchat_classification/chitchat_classification/Mix_5sentences3/experiment_dataset2/chitchat_test_dataset.csv'
test_df = pd.read_csv(test_path) 
# test_df=test_df[:10]




from torch.utils.data import Dataset, DataLoader

class Dataset(Dataset):
    def __init__(self, df):
        self.df = df

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        text = self.df.iloc[idx, 0]
        #         print(text)

        label = self.df.iloc[idx, 1]
        #         print(label)
        return text, label


test_dataset = Dataset(test_df)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)

import torch.nn.functional as F
from tqdm import tqdm

def test():
    model.eval()
    
    total_loss = 0
    total_len = 0
    total_correct = 0
    
    global predicted_labels
    global true_labels

    predicted_labels=[]
    true_labels=[]    
    for text, label in tqdm(test_loader):
        # print("Running Validation...")
        true_labels.append(label)
#         print('label', label)
        
        encoded_list = [tokenizer.encode(t, add_special_tokens=True) for t in text]
        padded_list = [e + [0] * (510 - len(e)) for e in encoded_list]

        sample = torch.tensor(padded_list)
        sample, label = sample.to(device), label.to(device)
        labels = label.clone().detach()
       
        with torch.no_grad():
            outputs = model(sample, labels=labels)
        _, logits = outputs

        pred = torch.argmax(F.softmax(logits, dim=1), dim=1)
        correct = pred.eq(labels)
        total_correct += correct.sum().item()
        total_len += len(labels)
        
        pred = pred.detach().cpu().numpy()
        predicted_labels.append(pred)
#         print('pred', pred)

        
    return true_labels, predicted_labels
#     return label, pred, predicted_labels, true_labels

    print('Test accuracy: {:.4f}'.format(total_correct / total_len))
    
    now = time.gmtime(time.time() - start_time)
    print('{} hours {} mins {} secs for training'.format(now.tm_hour, now.tm_min, now.tm_sec))
test()


len(true_labels), len(predicted_labels)

cf_true_labels=[]
cf_predicted_labels=[]

for labels, preds in zip(true_labels, predicted_labels):    
    for label, pred in zip(labels, preds):
        label=label.tolist()
        cf_true_labels.append(label)
        cf_predicted_labels.append(pred)

# cf_true_labels, cf_predicted_labels


import numpy as np

num_label = 2
num_pred = 2

confusion_matrix = np.zeros((num_label, num_pred), dtype = "int32") #제로 에러이 생성
# print('target_pred_counting', target_pred_counting)
idx_to_chr = {0:'칫챗', 1:'넌칫챗'}
chr_to_idx = {'칫챗':0 , '넌칫챗':1}

for cf_label, cf_pred in zip(cf_true_labels, cf_predicted_labels):
    confusion_matrix[cf_label][cf_pred] +=1
    
print(confusion_matrix)
# sum(confusion_matrix)    
# np.sum(confusion_matrix)
#[[60000     0]
# [  240 59760]]

cf_matrix=confusion_matrix

import numpy as np
from numpy import linalg as LA

#노멀라이제이션
# 1. 혼동행렬(Array상태)의 가로방향별로 각 input별 합을 구한다
total=np.sum(cf_matrix, axis=1)
print('total', total) 


total[:, None]

cf_matrix=cf_matrix/total
cf_matrix

# 노멀라이제이션 히트맵그리기
import seaborn as sns
import pandas as pd

df_cm = pd.DataFrame(cf_matrix, index=['Chitchat', 'Non-CHitchat'], columns=['Chitchat', 'Non-CHitchat'])
df_cm

import matplotlib.pyplot as plt


plt.figure(figsize=(10,7))
plt.title("Chitchat_ConfusionMatrix")
ax=sns.heatmap(df_cm,  fmt='.2%', cmap='Blues', annot=True,  annot_kws={"size": 15})
# ax.figure.axes[-1].yaxis.label.set_size(70)
plt.ylabel('True Class')
plt.xlabel('Predicted Class')
plt.xticks(fontsize =10)
plt.yticks(fontsize =10)
plt.setp(ax.get_xticklabels(), fontsize =12) #x축레이블
plt.setp(ax.get_yticklabels(), fontsize =12) #x축레이블
# ax.set_ylabel
# ax = sns.heatmap(data)
# ax.figure.axes[-1].set_ylabel('Accuracy %', size=10)


# 틱 레이블 글꼴 크기를 설정하는 plt.xticks(fontsize =)
# 틱 레이블 글꼴 크기를 설정하는 ax.set_xticklabels (xlabels, fontsize =)
# 틱 레이블 글꼴 크기를 설정하는 plt.setp(ax.get_xticklabels(), fontsize =)
# 틱 레이블 글꼴 크기를 설정하는 ax.tick_params (axis = 'x', labelsize =)        
                                    
                                  





